# langchain_rag
My first attempt to use LangChain for RAG.
The backend LLMs are local or remote running either on LM Studio on Ollama
Using machine with GPU or NPU wouild help to reduce the response wait to below a minutes.
Otherwise, a pure CPU running local LLM would require waiting time of several minutes for a response.

The diary folder contains .md diary text entries generated by Bing chat of a fictious Singaporean male.
The python script uses Chroma DB to store and retrieve vectorized senstence embeddings. Before storing the
vectors, the diary entries are splitted up into chunks then vectorized.

The retriver would be able to convert the query into embeddings and check against the Chroma DB for the closest
related vectors then obtain the relevant chunks.

These chunks are then used as context and send along with the user query merged in the prompt template to the LLM.
I have tested on QuantFactory - Meta Llama 3 Instruct 7B Q5_K_M.guff model. Result is satisfactory for the simple document repository I tested on but the quality may suffer on complicated documents. 
